{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Windows User / win用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: chcp: command not found\n"
     ]
    }
   ],
   "source": [
    "!chcp 65001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Linux User / Linux 用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_US.UTF-8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|█████████████████████████| 7/7 [00:00<00:00, 6320.80it/s]\n",
      "/home/leo/Desktop/leo-ext/self/other/fish-speech/checkpoints/fish-speech-1.5\n"
     ]
    }
   ],
   "source": [
    "# For Chinese users, you probably want to use mirror to accelerate downloading\n",
    "# !set HF_ENDPOINT=https://hf-mirror.com\n",
    "# !export HF_ENDPOINT=https://hf-mirror.com \n",
    "\n",
    "!huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebUI Inference\n",
    "\n",
    "> You can use --compile to fuse CUDA kernels for faster inference (10x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-15 10:01:00.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading Llama model...\u001b[0m\n",
      "\u001b[32m2025-05-15 10:01:06.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m681\u001b[0m - \u001b[1mRestored model from checkpoint\u001b[0m\n",
      "\u001b[32m2025-05-15 10:01:06.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m687\u001b[0m - \u001b[1mUsing DualARTransformer\u001b[0m\n",
      "\u001b[32m2025-05-15 10:01:06.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m695\u001b[0m - \u001b[1mCompiling function...\u001b[0m\n",
      "\u001b[32m2025-05-15 10:01:06.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mLoading VQ-GAN model...\u001b[0m\n",
      "/home/leo/Desktop/leo-ext/self/other/fish-speech/.venv/lib/python3.11/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:445: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/leo/Desktop/leo-ext/self/other/fish-speech/.venv/lib/python3.11/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:630: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/leo/Desktop/leo-ext/self/other/fish-speech/.venv/lib/python3.11/site-packages/vector_quantize_pytorch/finite_scalar_quantization.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/leo/Desktop/leo-ext/self/other/fish-speech/.venv/lib/python3.11/site-packages/vector_quantize_pytorch/lookup_free_quantization.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\u001b[32m2025-05-15 10:01:07.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.vqgan.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mLoaded model: <All keys matched successfully>\u001b[0m\n",
      "\u001b[32m2025-05-15 10:01:07.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mDecoder model loaded, warming up...\u001b[0m\n",
      "\u001b[32m2025-05-15 10:01:07.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hello world.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:01:07.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
      "  0%|                                                  | 0/1023 [00:00<?, ?it/s]/home/leo/.pyenv/versions/3.11.11/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  0%|                                       | 1/1023 [01:02<17:41:51, 62.34s/it]/home/leo/.pyenv/versions/3.11.11/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  0%|                                        | 2/1023 [01:02<7:19:54, 25.85s/it]/home/leo/.pyenv/versions/3.11.11/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  2%|▊                                        | 21/1023 [01:02<50:03,  3.00s/it]\n",
      "\u001b[32m2025-05-15 10:02:10.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 63.20 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:02:10.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 23 tokens in 63.20 seconds, 0.36 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:02:10.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 0.23 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:02:10.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 1.97 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:02:10.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 22])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:02:11.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mWarming up done, launching the web UI...\u001b[0m\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "\u001b[32m2025-05-15 10:02:37.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: hello!  how are you?\u001b[0m\n",
      "\u001b[32m2025-05-15 10:02:37.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
      "  0%|                                                  | 0/8164 [00:00<?, ?it/s]/home/leo/.pyenv/versions/3.11.11/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  0%|▏                                        | 39/8164 [00:00<02:05, 64.64it/s]\n",
      "\u001b[32m2025-05-15 10:02:37.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 0.65 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:02:37.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 41 tokens in 0.65 seconds, 63.07 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:02:37.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 40.23 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:02:37.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.01 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:02:37.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 40])\u001b[0m\n",
      "/home/leo/Desktop/leo-ext/self/other/fish-speech/.venv/lib/python3.11/site-packages/gradio/processing_utils.py:749: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "\u001b[32m2025-05-15 10:03:49.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you are doing great,\n",
      "let me set the context for this video,\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:49.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: I will be discussing about what\n",
      "are requests and responses\n",
      "\n",
      "and understand how client and server interact with each other using requests and responses\n",
      "first lets understad what client and server are,\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:49.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: client -> is a frontend user accesable interface ( say a website or app )\n",
      "server -> is a backend resource providing service ( api + database )\n",
      "\n",
      "just for the ease of understanding,\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:49.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: cosider you visited a restaruant and in that case you will be the client and ordering desk will be the server\n",
      "Now, let's understand how client and server communicate with each other.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:49.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order at a restaurant:\n",
      "1. You make a REQUEST by telling the waiter or at desk what food you want\n",
      "2.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:49.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The waiter brings back a RESPONSE with your food\n",
      "\n",
      "Similarly in web:\n",
      "- When you interact with a website (client), it sends REQUESTS to the server\n",
      "- The server processes these requests and sends back\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:49.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: RESPONSES\n",
      "\n",
      "Types of common requests:\n",
      "GET - when you want to fetch/receive information\n",
      "POST - when you want to send/create new information\n",
      "PUT - when you want to update existing information\n",
      "DELETE -\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:49.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: when you want to remove information\n",
      "\n",
      "Just like in a restaurant:\n",
      "GET - Looking at the menu\n",
      "POST - Placing a new order\n",
      "PUT - Modifying your existing order\n",
      "DELETE - Canceling your order\n",
      "\n",
      "Each request\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:49.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: and response has:\n",
      "- Headers (like order details/instructions)\n",
      "- Body (the actual data/food being transferred)\n",
      "- Status codes (success/failure messages)\n",
      "\n",
      "This request-response cycle is the foundation\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:49.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: of how the internet works and how applications communicate with servers.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:49.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/10 of sample 1/1\u001b[0m\n",
      "  1%|▍                                        | 91/8152 [00:01<02:03, 65.44it/s]\n",
      "\u001b[32m2025-05-15 10:03:51.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 1.44 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:51.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 93 tokens in 1.44 seconds, 64.64 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:51.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 41.23 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:51.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.03 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:51.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/10 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:51.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 92])\u001b[0m\n",
      "  4%|█▍                                      | 296/8011 [00:04<01:58, 65.06it/s]\n",
      "\u001b[32m2025-05-15 10:03:55.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 298 tokens in 4.66 seconds, 63.93 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:55.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 40.78 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:55.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.08 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:55.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/10 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:03:55.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 297])\u001b[0m\n",
      "  4%|█▍                                      | 285/7660 [00:04<01:58, 62.11it/s]\n",
      "\u001b[32m2025-05-15 10:04:00.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 287 tokens in 4.72 seconds, 60.76 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:00.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 38.76 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:00.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.35 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:00.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/10 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:00.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 286])\u001b[0m\n",
      "  4%|█▍                                      | 259/7324 [00:03<01:48, 65.08it/s]\n",
      "\u001b[32m2025-05-15 10:04:04.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 261 tokens in 4.38 seconds, 59.64 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:04.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 38.05 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:04.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.35 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:04.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/10 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:04.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 260])\u001b[0m\n",
      "  2%|▊                                       | 149/7021 [00:02<01:46, 64.63it/s]\n",
      "\u001b[32m2025-05-15 10:04:07.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 151 tokens in 2.73 seconds, 55.23 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:07.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.23 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:07.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.35 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:07.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/10 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:07.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 150])\u001b[0m\n",
      "  4%|█▊                                      | 304/6807 [00:04<01:40, 64.64it/s]\n",
      "\u001b[32m2025-05-15 10:04:12.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 306 tokens in 5.06 seconds, 60.43 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:12.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 38.55 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:12.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.35 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:12.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/10 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:12.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 305])\u001b[0m\n",
      "  5%|█▉                                      | 304/6434 [00:04<01:34, 64.68it/s]\n",
      "\u001b[32m2025-05-15 10:04:17.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 306 tokens in 5.26 seconds, 58.20 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:17.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.13 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:17.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:17.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/10 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:17.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 305])\u001b[0m\n",
      "  4%|█▋                                      | 255/6063 [00:03<01:29, 64.62it/s]\n",
      "\u001b[32m2025-05-15 10:04:22.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 257 tokens in 4.57 seconds, 56.22 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:22.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.86 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:22.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:22.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/10 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:22.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 256])\u001b[0m\n",
      "  5%|██                                      | 301/5743 [00:04<01:24, 64.58it/s]\n",
      "\u001b[32m2025-05-15 10:04:27.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 303 tokens in 5.31 seconds, 57.01 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:27.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.37 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:27.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:27.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/10 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:27.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 302])\u001b[0m\n",
      "  2%|▋                                        | 92/5419 [00:01<01:23, 64.00it/s]\n",
      "\u001b[32m2025-05-15 10:04:30.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 94 tokens in 2.19 seconds, 42.93 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:30.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 27.39 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:30.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:04:30.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 93])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone! Hope you're doing great.\n",
      "\n",
      "Let me set the context for this video.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: I'll be discussing what requests and responses are — and we'll understand how clients and servers interact with each other using these concepts.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's understand what \"client\" and \"server\" mean:\n",
      "\n",
      "Client → This is the front-end, user-accessible interface — such as a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Server → This is the back-end resource that provides services — like APIs and databases.\n",
      "\n",
      "To make this easier to grasp, consider this simple analogy: imagine you visit a restaurant.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: In that case:\n",
      "\n",
      "You are the client.\n",
      "\n",
      "The ordering desk is the server.\n",
      "\n",
      "Now, let's break down how the client and server communicate with each other.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order at a restaurant:\n",
      "\n",
      "You make a REQUEST by telling the waiter (or the person at the desk) what food you want.\n",
      "\n",
      "The waiter brings back a RESPONSE — your food.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Similarly, in the web world:\n",
      "\n",
      "When you interact with a website (the client), it sends a REQUEST to the server.\n",
      "\n",
      "The server processes this request and sends back a RESPONSE.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Let's look at some common types of requests:\n",
      "\n",
      "GET → Used when you want to fetch or receive information.\n",
      "\n",
      "POST → Used when you want to send or create new information.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT → Used when you want to update existing information.\n",
      "\n",
      "DELETE → Used when you want to remove information.\n",
      "\n",
      "To tie this back to our restaurant example:\n",
      "\n",
      "GET → Looking at the menu.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: POST → Placing a new order.\n",
      "\n",
      "PUT → Modifying your existing order.\n",
      "\n",
      "DELETE → Canceling your order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Each request and response typically consists of:\n",
      "\n",
      "Headers → Think of these like order details or special instructions.\n",
      "\n",
      "Body → This contains the actual data — like the food being transferred.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Status codes → These tell you if the request was successful or if it failed — like “Order received,” or “We're out of that item.”\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:30.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      "  1%|▍                                        | 93/8151 [00:01<02:03, 65.41it/s]\n",
      "\u001b[32m2025-05-15 10:06:32.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 1.47 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:32.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 95 tokens in 1.47 seconds, 64.69 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:32.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 41.26 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:32.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:32.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:32.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 94])\u001b[0m\n",
      "  2%|▊                                       | 174/8021 [00:02<02:00, 65.33it/s]\n",
      "\u001b[32m2025-05-15 10:06:34.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 176 tokens in 2.79 seconds, 63.02 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:34.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 40.20 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:34.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:34.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:34.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 175])\u001b[0m\n",
      "  3%|█                                       | 209/7793 [00:03<01:55, 65.40it/s]\n",
      "\u001b[32m2025-05-15 10:06:38.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 211 tokens in 3.43 seconds, 61.60 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:38.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 39.30 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:38.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:38.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:38.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 210])\u001b[0m\n",
      "  3%|█▍                                      | 263/7532 [00:04<01:51, 65.25it/s]\n",
      "\u001b[32m2025-05-15 10:06:42.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 265 tokens in 4.32 seconds, 61.29 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:42.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 39.10 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:42.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:42.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:42.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 264])\u001b[0m\n",
      "  2%|▉                                       | 176/7220 [00:02<01:48, 64.97it/s]\n",
      "\u001b[32m2025-05-15 10:06:45.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 178 tokens in 3.10 seconds, 57.46 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:45.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.65 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:45.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:45.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:45.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 177])\u001b[0m\n",
      "  3%|█▏                                      | 206/6982 [00:03<01:44, 64.95it/s]\n",
      "\u001b[32m2025-05-15 10:06:49.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 208 tokens in 3.52 seconds, 59.07 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:49.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.68 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:49.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:49.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:49.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 207])\u001b[0m\n",
      "  3%|█▎                                      | 219/6715 [00:03<01:40, 64.62it/s]\n",
      "\u001b[32m2025-05-15 10:06:53.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 221 tokens in 3.82 seconds, 57.86 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:53.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.91 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:53.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:53.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:53.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 220])\u001b[0m\n",
      "  3%|█▏                                      | 201/6442 [00:03<01:36, 64.59it/s]\n",
      "\u001b[32m2025-05-15 10:06:56.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 203 tokens in 3.60 seconds, 56.43 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:56.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.00 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:56.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:56.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:06:56.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 202])\u001b[0m\n",
      "  4%|█▍                                      | 223/6179 [00:03<01:32, 64.73it/s]\n",
      "\u001b[32m2025-05-15 10:07:00.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 225 tokens in 3.97 seconds, 56.71 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:00.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.18 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:00.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:00.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:00.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 224])\u001b[0m\n",
      "  3%|█                                       | 162/5912 [00:02<01:29, 64.48it/s]\n",
      "\u001b[32m2025-05-15 10:07:03.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 164 tokens in 3.10 seconds, 52.89 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:03.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.74 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:03.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:03.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:03.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 163])\u001b[0m\n",
      "  3%|█▍                                      | 196/5699 [00:03<01:25, 64.43it/s]\n",
      "\u001b[32m2025-05-15 10:07:07.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 198 tokens in 3.61 seconds, 54.78 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:07.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.95 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:07.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:07.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:07.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 197])\u001b[0m\n",
      "  3%|█▏                                      | 166/5458 [00:02<01:22, 64.48it/s]\n",
      "\u001b[32m2025-05-15 10:07:10.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 168 tokens in 3.23 seconds, 52.05 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:10.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.20 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:10.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.41 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:07:10.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 167])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 239.26 seconds\u001b[0m\n",
      "/home/leo/Desktop/leo-ext/self/other/fish-speech/.venv/lib/python3.11/site-packages/vector_quantize_pytorch/residual_fsq.py:170: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled = False):\n",
      "\u001b[32m2025-05-15 10:11:29.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 5152])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone! Hope you're doing great.\n",
      "\n",
      "Let me set the context for this video.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: I'll be discussing what requests and responses are — and we'll understand how clients and servers interact with each other using these concepts.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's understand what \"client\" and \"server\" mean:\n",
      "\n",
      "Client → This is the front-end, user-accessible interface — such as a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Server → This is the back-end resource that provides services — like APIs and databases.\n",
      "\n",
      "To make this easier to grasp, consider this simple analogy: imagine you visit a restaurant.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: In that case:\n",
      "\n",
      "You are the client.\n",
      "\n",
      "The ordering desk is the server.\n",
      "\n",
      "Now, let's break down how the client and server communicate with each other.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order at a restaurant:\n",
      "\n",
      "You make a REQUEST by telling the waiter (or the person at the desk) what food you want.\n",
      "\n",
      "The waiter brings back a RESPONSE — your food.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Similarly, in the web world:\n",
      "\n",
      "When you interact with a website (the client), it sends a REQUEST to the server.\n",
      "\n",
      "The server processes this request and sends back a RESPONSE.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Let's look at some common types of requests:\n",
      "\n",
      "GET → Used when you want to fetch or receive information.\n",
      "\n",
      "POST → Used when you want to send or create new information.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT → Used when you want to update existing information.\n",
      "\n",
      "DELETE → Used when you want to remove information.\n",
      "\n",
      "To tie this back to our restaurant example:\n",
      "\n",
      "GET → Looking at the menu.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: POST → Placing a new order.\n",
      "\n",
      "PUT → Modifying your existing order.\n",
      "\n",
      "DELETE → Canceling your order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Each request and response typically consists of:\n",
      "\n",
      "Headers → Think of these like order details or special instructions.\n",
      "\n",
      "Body → This contains the actual data — like the food being transferred.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Status codes → These tell you if the request was successful or if it failed — like “Order received,” or “We're out of that item.”\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:29.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      "  3%|█▎                                       | 93/2989 [00:01<00:44, 64.95it/s]\n",
      "\u001b[32m2025-05-15 10:11:32.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 2.36 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:32.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 95 tokens in 2.36 seconds, 40.20 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:32.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 25.64 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:32.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.58 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:32.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:32.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 94])\u001b[0m\n",
      "  7%|██▌                                     | 178/2735 [00:02<00:39, 64.65it/s]\n",
      "\u001b[32m2025-05-15 10:11:36.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 180 tokens in 3.84 seconds, 46.86 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:36.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 29.89 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:36.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.59 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:36.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:11:36.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 179])\u001b[0m\n",
      "100%|███████████████████████████████████████| 2503/2503 [00:39<00:00, 64.18it/s]\n",
      "\u001b[32m2025-05-15 10:12:16.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 2504 tokens in 40.20 seconds, 62.29 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:16.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 39.74 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:16.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.59 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:16.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:16.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 2503])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 14.96 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 322])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone! Hope you're doing great.\n",
      "\n",
      "Let me set the context for this video.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: I'll be discussing what requests and responses are — and we'll understand how clients and servers interact with each other using these concepts.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's understand what \"client\" and \"server\" mean:\n",
      "\n",
      "Client → This is the front-end, user-accessible interface — such as a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Server → This is the back-end resource that provides services — like APIs and databases.\n",
      "\n",
      "To make this easier to grasp, consider this simple analogy: imagine you visit a restaurant.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: In that case:\n",
      "\n",
      "You are the client.\n",
      "\n",
      "The ordering desk is the server.\n",
      "\n",
      "Now, let's break down how the client and server communicate with each other.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order at a restaurant:\n",
      "\n",
      "You make a REQUEST by telling the waiter (or the person at the desk) what food you want.\n",
      "\n",
      "The waiter brings back a RESPONSE — your food.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Similarly, in the web world:\n",
      "\n",
      "When you interact with a website (the client), it sends a REQUEST to the server.\n",
      "\n",
      "The server processes this request and sends back a RESPONSE.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Let's look at some common types of requests:\n",
      "\n",
      "GET → Used when you want to fetch or receive information.\n",
      "\n",
      "POST → Used when you want to send or create new information.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT → Used when you want to update existing information.\n",
      "\n",
      "DELETE → Used when you want to remove information.\n",
      "\n",
      "To tie this back to our restaurant example:\n",
      "\n",
      "GET → Looking at the menu.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: POST → Placing a new order.\n",
      "\n",
      "PUT → Modifying your existing order.\n",
      "\n",
      "DELETE → Canceling your order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Each request and response typically consists of:\n",
      "\n",
      "Headers → Think of these like order details or special instructions.\n",
      "\n",
      "Body → This contains the actual data — like the food being transferred.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Status codes → These tell you if the request was successful or if it failed — like “Order received,” or “We're out of that item.”\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:42.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      "  1%|▌                                        | 97/7819 [00:01<02:00, 63.89it/s]\n",
      "\u001b[32m2025-05-15 10:12:44.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 1.60 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:44.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 99 tokens in 1.61 seconds, 61.68 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:44.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 39.35 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:44.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:44.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:44.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 98])\u001b[0m\n",
      "  2%|▊                                       | 155/7685 [00:02<01:58, 63.61it/s]\n",
      "\u001b[32m2025-05-15 10:12:46.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 157 tokens in 2.62 seconds, 59.84 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:46.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 38.17 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:46.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:46.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:46.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 156])\u001b[0m\n",
      "  2%|▉                                       | 180/7476 [00:02<01:54, 63.83it/s]\n",
      "\u001b[32m2025-05-15 10:12:49.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 182 tokens in 3.09 seconds, 58.98 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:49.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.63 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:49.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:49.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:49.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 181])\u001b[0m\n",
      "  3%|█▎                                      | 247/7244 [00:03<01:49, 63.66it/s]\n",
      "\u001b[32m2025-05-15 10:12:53.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 249 tokens in 4.20 seconds, 59.32 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:53.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.84 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:53.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:53.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:53.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 248])\u001b[0m\n",
      "  4%|█▍                                      | 252/6948 [00:03<01:45, 63.70it/s]\n",
      "\u001b[32m2025-05-15 10:12:58.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 254 tokens in 4.38 seconds, 57.97 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:58.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.98 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:58.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:58.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:12:58.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 253])\u001b[0m\n",
      "  1%|▌                                        | 84/6634 [00:01<01:43, 63.14it/s]\n",
      "\u001b[32m2025-05-15 10:13:00.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 86 tokens in 1.82 seconds, 47.14 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:00.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 30.07 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:00.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:00.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:00.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 85])\u001b[0m\n",
      "  3%|█▎                                      | 207/6489 [00:03<01:38, 63.59it/s]\n",
      "\u001b[32m2025-05-15 10:13:03.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 209 tokens in 3.64 seconds, 57.35 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:03.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.58 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:03.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:03.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:03.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 208])\u001b[0m\n",
      "  4%|█▋                                      | 258/6228 [00:04<01:33, 63.60it/s]\n",
      "\u001b[32m2025-05-15 10:13:08.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 260 tokens in 4.59 seconds, 56.71 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:08.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.17 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:08.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:08.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:08.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 259])\u001b[0m\n",
      "  3%|█▍                                      | 206/5908 [00:03<01:29, 63.64it/s]\n",
      "\u001b[32m2025-05-15 10:13:12.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 208 tokens in 3.87 seconds, 53.69 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:12.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.25 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:12.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:12.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:12.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 207])\u001b[0m\n",
      "  3%|█                                       | 152/5658 [00:02<01:26, 63.29it/s]\n",
      "\u001b[32m2025-05-15 10:13:15.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 154 tokens in 3.03 seconds, 50.77 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:15.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 32.39 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:15.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:15.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:15.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 153])\u001b[0m\n",
      "  5%|█▉                                      | 265/5455 [00:04<01:21, 63.80it/s]\n",
      "\u001b[32m2025-05-15 10:13:20.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 267 tokens in 4.79 seconds, 55.78 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:20.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.59 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:20.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:20.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:20.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 266])\u001b[0m\n",
      "  3%|█▏                                      | 152/5145 [00:02<01:19, 63.17it/s]\n",
      "\u001b[32m2025-05-15 10:13:23.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 154 tokens in 3.19 seconds, 48.26 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:23.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 30.79 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:23.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:13:23.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 153])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you're doing great. In this video, I'm going to walk you through a basic but important concept—requests and responses, and how they help clients and servers communicate.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's get clear on what a client and server actually are. A client is what the user interacts with—like a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The server is the backend system that provides services, like APIs and databases. To make it simple, think of a restaurant. You're the client, and the ordering desk or waiter is the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order, you're making a request. When the food comes back, that's a response. That same idea applies to how the web works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you click on something or enter data on a website, the client sends a request to the server. The server processes it and sends back a response.\n",
      "\n",
      "There are a few common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A GET request is when you want to receive or fetch some information. A POST request is used when you want to send or create something new.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is for updating existing data, and DELETE is when you want to remove something. Going back to the restaurant example—GET is like reading the menu. POST is placing a new order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is changing an existing order, and DELETE is canceling the order. Each of these requests usually includes a few key parts. There are headers, which carry instructions or important details.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: There's also a body, which contains the actual data being sent. And finally, there are status codes, which tell you if the request was successful or if something went wrong.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: This entire cycle of making requests and getting responses is what keeps the internet running.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Whether you're using a mobile app, browsing a site, or using a service online, this is how the client and server talk to each other. It's a simple pattern but very powerful.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Once you understand it, you'll have a much better idea of how applications work behind the scenes. Hope that gives you a clear picture.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:29.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      "  2%|▉                                       | 191/7799 [00:02<01:56, 65.11it/s]\n",
      "\u001b[32m2025-05-15 10:23:33.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 3.03 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:33.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 193 tokens in 3.03 seconds, 63.78 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:33.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 40.69 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:33.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:33.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:33.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 192])\u001b[0m\n",
      "  2%|▉                                       | 166/7565 [00:02<01:53, 65.03it/s]\n",
      "\u001b[32m2025-05-15 10:23:35.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 168 tokens in 2.82 seconds, 59.59 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:35.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 38.02 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:35.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:35.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:35.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 167])\u001b[0m\n",
      "  3%|█▎                                      | 231/7345 [00:03<01:49, 64.75it/s]\n",
      "\u001b[32m2025-05-15 10:23:39.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 233 tokens in 3.85 seconds, 60.48 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:39.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 38.58 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:39.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:39.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:39.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 232])\u001b[0m\n",
      "  2%|▉                                       | 170/7070 [00:02<01:47, 64.38it/s]\n",
      "\u001b[32m2025-05-15 10:23:42.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 172 tokens in 3.03 seconds, 56.76 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:42.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.21 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:42.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:42.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:42.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 171])\u001b[0m\n",
      "  3%|█▎                                      | 226/6847 [00:03<01:42, 64.49it/s]\n",
      "\u001b[32m2025-05-15 10:23:46.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 228 tokens in 3.88 seconds, 58.78 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:46.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.49 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:46.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:46.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:46.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 227])\u001b[0m\n",
      "  2%|▉                                       | 164/6579 [00:02<01:39, 64.22it/s]\n",
      "\u001b[32m2025-05-15 10:23:49.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 166 tokens in 3.03 seconds, 54.83 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:49.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.98 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:49.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:49.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:49.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 165])\u001b[0m\n",
      "  3%|█▎                                      | 218/6358 [00:03<01:35, 64.22it/s]\n",
      "\u001b[32m2025-05-15 10:23:53.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 220 tokens in 3.85 seconds, 57.08 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:53.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.41 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:53.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:53.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:53.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 219])\u001b[0m\n",
      "  4%|█▌                                      | 240/6087 [00:03<01:31, 64.15it/s]\n",
      "\u001b[32m2025-05-15 10:23:57.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 242 tokens in 4.30 seconds, 56.32 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:57.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.93 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:57.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:57.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:23:57.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 241])\u001b[0m\n",
      "  3%|█▍                                      | 200/5800 [00:03<01:27, 63.98it/s]\n",
      "\u001b[32m2025-05-15 10:24:01.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 202 tokens in 3.77 seconds, 53.60 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:01.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.19 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:01.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:01.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:01.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 201])\u001b[0m\n",
      "  2%|▊                                       | 114/5573 [00:01<01:25, 63.68it/s]\n",
      "\u001b[32m2025-05-15 10:24:03.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 116 tokens in 2.43 seconds, 47.72 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:03.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 30.44 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:03.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:03.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:03.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 115])\u001b[0m\n",
      "  4%|█▌                                      | 203/5408 [00:03<01:21, 64.06it/s]\n",
      "\u001b[32m2025-05-15 10:24:07.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 205 tokens in 3.78 seconds, 54.30 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:07.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.64 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:07.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:07.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:07.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 204])\u001b[0m\n",
      "  3%|█▏                                      | 153/5166 [00:02<01:18, 63.96it/s]\n",
      "\u001b[32m2025-05-15 10:24:10.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 155 tokens in 3.11 seconds, 49.80 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:10.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 31.77 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:10.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:10.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 154])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you're doing great. In this video, I'm going to walk you through a basic but important concept—requests and responses, and how they help clients and servers communicate.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's get clear on what a client and server actually are. A client is what the user interacts with—like a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The server is the backend system that provides services, like APIs and databases. To make it simple, think of a restaurant. You're the client, and the ordering desk or waiter is the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order, you're making a request. When the food comes back, that's a response. That same idea applies to how the web works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you click on something or enter data on a website, the client sends a request to the server. The server processes it and sends back a response.\n",
      "\n",
      "There are a few common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A GET request is when you want to receive or fetch some information. A POST request is used when you want to send or create something new.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is for updating existing data, and DELETE is when you want to remove something. Going back to the restaurant example—GET is like reading the menu. POST is placing a new order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is changing an existing order, and DELETE is canceling the order. Each of these requests usually includes a few key parts. There are headers, which carry instructions or important details.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: There's also a body, which contains the actual data being sent. And finally, there are status codes, which tell you if the request was successful or if something went wrong.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: This entire cycle of making requests and getting responses is what keeps the internet running.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Whether you're using a mobile app, browsing a site, or using a service online, this is how the client and server talk to each other. It's a simple pattern but very powerful.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Once you understand it, you'll have a much better idea of how applications work behind the scenes. Hope that gives you a clear picture.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:19.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      "  3%|█▏                                      | 231/7799 [00:03<01:57, 64.42it/s]\n",
      "\u001b[32m2025-05-15 10:24:22.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 3.68 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:22.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 233 tokens in 3.68 seconds, 63.31 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:22.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 40.38 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:22.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:22.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:22.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 232])\u001b[0m\n",
      "  2%|▉                                       | 177/7525 [00:02<01:54, 63.99it/s]\n",
      "\u001b[32m2025-05-15 10:24:25.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 179 tokens in 3.08 seconds, 58.06 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:25.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.04 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:25.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:25.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:25.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 178])\u001b[0m\n",
      "  3%|█▎                                      | 237/7294 [00:03<01:50, 63.94it/s]\n",
      "\u001b[32m2025-05-15 10:24:29.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 239 tokens in 4.02 seconds, 59.47 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:29.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.94 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:29.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:29.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:29.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 238])\u001b[0m\n",
      "  2%|▉                                       | 163/7013 [00:02<01:47, 63.83it/s]\n",
      "\u001b[32m2025-05-15 10:24:32.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 165 tokens in 2.97 seconds, 55.55 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:32.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.44 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:32.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:32.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:32.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 164])\u001b[0m\n",
      "  4%|█▍                                      | 239/6797 [00:03<01:42, 63.77it/s]\n",
      "\u001b[32m2025-05-15 10:24:37.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 241 tokens in 4.12 seconds, 58.43 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:37.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.27 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:37.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:37.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:37.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 240])\u001b[0m\n",
      "  2%|▉                                       | 160/6516 [00:02<01:40, 63.55it/s]\n",
      "\u001b[32m2025-05-15 10:24:40.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 162 tokens in 3.02 seconds, 53.72 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:40.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.27 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:40.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:40.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:40.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 161])\u001b[0m\n",
      "  3%|█▎                                      | 213/6299 [00:03<01:35, 63.74it/s]\n",
      "\u001b[32m2025-05-15 10:24:43.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 215 tokens in 3.81 seconds, 56.47 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:43.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.02 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:43.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:43.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:43.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 214])\u001b[0m\n",
      "  4%|█▋                                      | 251/6033 [00:03<01:30, 63.77it/s]\n",
      "\u001b[32m2025-05-15 10:24:48.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 253 tokens in 4.50 seconds, 56.19 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:48.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.85 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:48.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:48.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:48.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 252])\u001b[0m\n",
      "  4%|█▍                                      | 206/5735 [00:03<01:26, 63.71it/s]\n",
      "\u001b[32m2025-05-15 10:24:52.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 208 tokens in 3.90 seconds, 53.36 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:52.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.04 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:52.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:52.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:52.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 207])\u001b[0m\n",
      "  2%|▊                                       | 114/5502 [00:01<01:24, 63.49it/s]\n",
      "\u001b[32m2025-05-15 10:24:54.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 116 tokens in 2.46 seconds, 47.07 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:54.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 30.03 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:54.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:54.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:54.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 115])\u001b[0m\n",
      "  4%|█▌                                      | 204/5337 [00:03<01:20, 63.68it/s]\n",
      "\u001b[32m2025-05-15 10:24:58.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 206 tokens in 3.82 seconds, 53.90 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:58.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.38 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:58.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:58.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:24:58.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 205])\u001b[0m\n",
      "  3%|█▏                                      | 149/5094 [00:02<01:17, 63.43it/s]\n",
      "\u001b[32m2025-05-15 10:25:01.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 151 tokens in 3.09 seconds, 48.82 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:25:01.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 31.14 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:25:01.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:25:01.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 150])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 21.38 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 461])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you're doing great. In this video, I'm going to walk you through a basic but important concept—requests and responses, and how they help clients and servers communicate.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's get clear on what a client and server actually are. A client is what the user interacts with—like a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The server is the backend system that provides services, like APIs and databases. To make it simple, think of a restaurant. You're the client, and the ordering desk or waiter is the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order, you're making a request. When the food comes back, that's a response. That same idea applies to how the web works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you click on something or enter data on a website, the client sends a request to the server. The server processes it and sends back a response.\n",
      "\n",
      "There are a few common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A GET request is when you want to receive or fetch some information. A POST request is used when you want to send or create something new.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is for updating existing data, and DELETE is when you want to remove something. Going back to the restaurant example—GET is like reading the menu. POST is placing a new order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is changing an existing order, and DELETE is canceling the order. Each of these requests usually includes a few key parts. There are headers, which carry instructions or important details.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: There's also a body, which contains the actual data being sent. And finally, there are status codes, which tell you if the request was successful or if something went wrong.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: This entire cycle of making requests and getting responses is what keeps the internet running.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Whether you're using a mobile app, browsing a site, or using a service online, this is how the client and server talk to each other. It's a simple pattern but very powerful.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Once you understand it, you'll have a much better idea of how applications work behind the scenes. Hope that gives you a clear picture.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:06.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      "  3%|█                                       | 205/7660 [00:03<01:56, 64.18it/s]\n",
      "\u001b[32m2025-05-15 10:30:09.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 3.30 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:09.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 207 tokens in 3.30 seconds, 62.72 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:09.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 40.01 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:09.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:09.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:09.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 206])\u001b[0m\n",
      "  2%|▉                                       | 165/7412 [00:02<01:52, 64.62it/s]\n",
      "\u001b[32m2025-05-15 10:30:12.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 167 tokens in 2.87 seconds, 58.25 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:12.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.16 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:12.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:12.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:12.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 166])\u001b[0m\n",
      "  3%|█▎                                      | 244/7193 [00:03<01:47, 64.58it/s]\n",
      "\u001b[32m2025-05-15 10:30:16.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 246 tokens in 4.09 seconds, 60.14 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:16.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 38.37 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:16.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:16.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:16.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 245])\u001b[0m\n",
      "  2%|▉                                       | 157/6905 [00:02<01:44, 64.39it/s]\n",
      "\u001b[32m2025-05-15 10:30:19.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 159 tokens in 2.88 seconds, 55.30 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:19.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.28 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:19.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:19.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:19.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 158])\u001b[0m\n",
      "  3%|█▎                                      | 229/6695 [00:03<01:40, 64.32it/s]\n",
      "\u001b[32m2025-05-15 10:30:23.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 231 tokens in 3.96 seconds, 58.40 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:23.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.25 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:23.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:23.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:23.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 230])\u001b[0m\n",
      "  3%|█                                       | 162/6424 [00:02<01:37, 64.40it/s]\n",
      "\u001b[32m2025-05-15 10:30:26.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 164 tokens in 3.02 seconds, 54.31 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:26.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.65 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:26.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:26.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:26.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 163])\u001b[0m\n",
      "  4%|█▍                                      | 229/6205 [00:03<01:32, 64.45it/s]\n",
      "\u001b[32m2025-05-15 10:30:30.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 231 tokens in 4.04 seconds, 57.17 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:30.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.47 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:30.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:30.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:30.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 230])\u001b[0m\n",
      "  4%|█▋                                      | 259/5923 [00:04<01:28, 64.13it/s]\n",
      "\u001b[32m2025-05-15 10:30:35.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 261 tokens in 4.64 seconds, 56.29 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:35.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.91 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:35.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:35.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:35.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 260])\u001b[0m\n",
      "  4%|█▍                                      | 198/5617 [00:03<01:24, 63.93it/s]\n",
      "\u001b[32m2025-05-15 10:30:38.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 200 tokens in 3.78 seconds, 52.88 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:38.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.73 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:38.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:38.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:38.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 199])\u001b[0m\n",
      "  2%|▊                                       | 115/5392 [00:01<01:22, 63.90it/s]\n",
      "\u001b[32m2025-05-15 10:30:41.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 117 tokens in 2.48 seconds, 47.26 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:41.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 30.15 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:41.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:41.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:41.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 116])\u001b[0m\n",
      "  4%|█▌                                      | 211/5226 [00:03<01:18, 63.98it/s]\n",
      "\u001b[32m2025-05-15 10:30:45.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 213 tokens in 3.94 seconds, 54.02 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:45.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.46 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:45.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:45.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:45.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 212])\u001b[0m\n",
      "  3%|█▏                                      | 152/5184 [00:02<01:18, 64.05it/s]\n",
      "\u001b[32m2025-05-15 10:30:48.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 154 tokens in 3.10 seconds, 49.60 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:48.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 31.64 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:48.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 4.45 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:30:48.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 153])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:29.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 1368.61 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:29.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 29471])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you're doing great. In this video, I'm going to walk you through a basic but important concept—requests and responses, and how they help clients and servers communicate.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's get clear on what a client and server actually are. A client is what the user interacts with—like a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The server is the backend system that provides services, like APIs and databases. To make it simple, think of a restaurant. You're the client, and the ordering desk or waiter is the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order, you're making a request. When the food comes back, that's a response. That same idea applies to how the web works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you click on something or enter data on a website, the client sends a request to the server. The server processes it and sends back a response.\n",
      "\n",
      "There are a few common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A GET request is when you want to receive or fetch some information. A POST request is used when you want to send or create something new.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is for updating existing data, and DELETE is when you want to remove something. Going back to the restaurant example—GET is like reading the menu. POST is placing a new order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is changing an existing order, and DELETE is canceling the order. Each of these requests usually includes a few key parts. There are headers, which carry instructions or important details.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: There's also a body, which contains the actual data being sent. And finally, there are status codes, which tell you if the request was successful or if something went wrong.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: This entire cycle of making requests and getting responses is what keeps the internet running.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Whether you're using a mobile app, browsing a site, or using a service online, this is how the client and server talk to each other. It's a simple pattern but very powerful.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Once you understand it, you'll have a much better idea of how applications work behind the scenes. Hope that gives you a clear picture.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:32.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:51.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 342.16 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:51.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 7368])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you're doing great. In this video, I'm going to walk you through a basic but important concept—requests and responses, and how they help clients and servers communicate.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's get clear on what a client and server actually are. A client is what the user interacts with—like a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The server is the backend system that provides services, like APIs and databases. To make it simple, think of a restaurant. You're the client, and the ordering desk or waiter is the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order, you're making a request. When the food comes back, that's a response. That same idea applies to how the web works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you click on something or enter data on a website, the client sends a request to the server. The server processes it and sends back a response.\n",
      "\n",
      "There are a few common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A GET request is when you want to receive or fetch some information. A POST request is used when you want to send or create something new.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is for updating existing data, and DELETE is when you want to remove something. Going back to the restaurant example—GET is like reading the menu. POST is placing a new order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is changing an existing order, and DELETE is canceling the order. Each of these requests usually includes a few key parts. There are headers, which carry instructions or important details.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: There's also a body, which contains the actual data being sent. And finally, there are status codes, which tell you if the request was successful or if something went wrong.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: This entire cycle of making requests and getting responses is what keeps the internet running.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Whether you're using a mobile app, browsing a site, or using a service online, this is how the client and server talk to each other. It's a simple pattern but very powerful.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Once you understand it, you'll have a much better idea of how applications work behind the scenes. Hope that gives you a clear picture.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:52.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      " 28%|███████████▎                             | 208/753 [00:03<00:08, 64.16it/s]\n",
      "\u001b[32m2025-05-15 10:32:57.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 4.62 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:57.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 210 tokens in 4.62 seconds, 45.49 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:57.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 29.02 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:57.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.42 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:57.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:32:57.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 209])\u001b[0m\n",
      "100%|█████████████████████████████████████████| 243/243 [00:03<00:00, 64.44it/s]\n",
      "\u001b[32m2025-05-15 10:33:02.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 244 tokens in 5.45 seconds, 44.73 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:02.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 28.53 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:02.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.42 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:02.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:02.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 243])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you're doing great. In this video, I'm going to walk you through a basic but important concept—requests and responses, and how they help clients and servers communicate.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's get clear on what a client and server actually are. A client is what the user interacts with—like a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The server is the backend system that provides services, like APIs and databases. To make it simple, think of a restaurant. You're the client, and the ordering desk or waiter is the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order, you're making a request. When the food comes back, that's a response. That same idea applies to how the web works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you click on something or enter data on a website, the client sends a request to the server. The server processes it and sends back a response.\n",
      "\n",
      "There are a few common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A GET request is when you want to receive or fetch some information. A POST request is used when you want to send or create something new.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is for updating existing data, and DELETE is when you want to remove something. Going back to the restaurant example—GET is like reading the menu. POST is placing a new order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is changing an existing order, and DELETE is canceling the order. Each of these requests usually includes a few key parts. There are headers, which carry instructions or important details.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: There's also a body, which contains the actual data being sent. And finally, there are status codes, which tell you if the request was successful or if something went wrong.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: This entire cycle of making requests and getting responses is what keeps the internet running.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Whether you're using a mobile app, browsing a site, or using a service online, this is how the client and server talk to each other. It's a simple pattern but very powerful.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Once you understand it, you'll have a much better idea of how applications work behind the scenes. Hope that gives you a clear picture.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:21.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      " 10%|████                                    | 305/2969 [00:04<00:41, 64.35it/s]\n",
      "\u001b[32m2025-05-15 10:33:27.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 5.70 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:27.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 307 tokens in 5.70 seconds, 53.89 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:27.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.38 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:27.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.42 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:27.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:27.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 306])\u001b[0m\n",
      "  9%|███▌                                    | 204/2265 [00:03<00:32, 63.91it/s]\n",
      "\u001b[32m2025-05-15 10:33:32.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 206 tokens in 4.58 seconds, 45.01 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:32.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 28.71 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:32.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.42 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:32.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:32.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 205])\u001b[0m\n",
      " 21%|████████▎                               | 414/2007 [00:06<00:24, 63.95it/s]\n",
      "\u001b[32m2025-05-15 10:33:39.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 416 tokens in 7.81 seconds, 53.24 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:39.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.96 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:39.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.42 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:39.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:39.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 415])\u001b[0m\n",
      " 16%|██████▌                                 | 253/1549 [00:03<00:20, 63.76it/s]\n",
      "\u001b[32m2025-05-15 10:33:45.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 255 tokens in 5.57 seconds, 45.75 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:45.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 29.18 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:45.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.42 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:45.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:45.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 254])\u001b[0m\n",
      " 26%|██████████▌                             | 327/1243 [00:05<00:14, 63.69it/s]\n",
      "\u001b[32m2025-05-15 10:33:52.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 329 tokens in 6.67 seconds, 49.29 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:52.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 31.44 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:52.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.42 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:52.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:52.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 328])\u001b[0m\n",
      " 41%|████████████████▌                        | 354/874 [00:05<00:08, 63.78it/s]\n",
      "\u001b[32m2025-05-15 10:33:59.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 356 tokens in 7.22 seconds, 49.29 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:59.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 31.44 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:59.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.42 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:59.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:33:59.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 355])\u001b[0m\n",
      " 84%|██████████████████████████████████▎      | 388/463 [00:06<00:01, 63.74it/s]\n",
      "\u001b[32m2025-05-15 10:34:07.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 390 tokens in 7.86 seconds, 49.64 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:34:07.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 31.67 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:34:07.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.42 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:34:07.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:34:07.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 389])\u001b[0m\n",
      "100%|███████████████████████████████████████████| 22/22 [00:00<00:00, 64.48it/s]\n",
      "\u001b[32m2025-05-15 10:34:09.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 23 tokens in 2.23 seconds, 10.32 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:34:09.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 6.58 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:34:09.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.42 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:34:09.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:34:09.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 22])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 186.32 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 4012])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you're doing great. In this video, I'm going to walk you through a basic but important concept—requests and responses, and how they help clients and servers communicate.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's get clear on what a client and server actually are. A client is what the user interacts with—like a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The server is the backend system that provides services, like APIs and databases. To make it simple, think of a restaurant. You're the client, and the ordering desk or waiter is the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order, you're making a request. When the food comes back, that's a response. That same idea applies to how the web works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you click on something or enter data on a website, the client sends a request to the server. The server processes it and sends back a response.\n",
      "\n",
      "There are a few common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A GET request is when you want to receive or fetch some information. A POST request is used when you want to send or create something new.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is for updating existing data, and DELETE is when you want to remove something. Going back to the restaurant example—GET is like reading the menu. POST is placing a new order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is changing an existing order, and DELETE is canceling the order. Each of these requests usually includes a few key parts. There are headers, which carry instructions or important details.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: There's also a body, which contains the actual data being sent. And finally, there are status codes, which tell you if the request was successful or if something went wrong.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: This entire cycle of making requests and getting responses is what keeps the internet running.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Whether you're using a mobile app, browsing a site, or using a service online, this is how the client and server talk to each other. It's a simple pattern but very powerful.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Once you understand it, you'll have a much better idea of how applications work behind the scenes. Hope that gives you a clear picture.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:13.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      " 12%|████▉                                   | 509/4109 [00:07<00:55, 64.68it/s]\n",
      "\u001b[32m2025-05-15 10:36:22.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 8.59 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:22.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 511 tokens in 8.59 seconds, 59.49 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:22.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.95 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:22.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:22.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:22.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 510])\u001b[0m\n",
      " 11%|████▏                                   | 318/2997 [00:04<00:41, 64.39it/s]\n",
      "\u001b[32m2025-05-15 10:36:28.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 320 tokens in 6.32 seconds, 50.60 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:28.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 32.28 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:28.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:28.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:28.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 319])\u001b[0m\n",
      " 16%|██████▍                                 | 420/2625 [00:06<00:34, 64.13it/s]\n",
      "\u001b[32m2025-05-15 10:36:36.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 422 tokens in 7.86 seconds, 53.68 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:36.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.24 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:36.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:36.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:36.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 421])\u001b[0m\n",
      " 11%|████▍                                   | 237/2161 [00:03<00:30, 63.92it/s]\n",
      "\u001b[32m2025-05-15 10:36:41.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 239 tokens in 5.20 seconds, 45.95 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:41.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 29.31 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:41.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:41.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:41.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 238])\u001b[0m\n",
      " 17%|██████▉                                 | 326/1871 [00:05<00:24, 63.94it/s]\n",
      "\u001b[32m2025-05-15 10:36:48.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 328 tokens in 6.50 seconds, 50.47 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:48.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 32.19 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:48.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:48.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:48.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 327])\u001b[0m\n",
      " 16%|██████▌                                 | 245/1503 [00:03<00:19, 63.90it/s]\n",
      "\u001b[32m2025-05-15 10:36:53.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 247 tokens in 5.38 seconds, 45.88 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:53.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 29.27 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:53.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:53.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:36:53.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 246])\u001b[0m\n",
      " 30%|███████████▉                            | 360/1201 [00:05<00:13, 64.03it/s]\n",
      "\u001b[32m2025-05-15 10:37:00.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 362 tokens in 7.16 seconds, 50.53 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:00.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 32.23 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:00.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:00.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:00.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 361])\u001b[0m\n",
      " 50%|████████████████████▎                    | 391/788 [00:06<00:06, 63.75it/s]\n",
      "\u001b[32m2025-05-15 10:37:08.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 393 tokens in 7.85 seconds, 50.08 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:08.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 31.95 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:08.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:08.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:08.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 392])\u001b[0m\n",
      "100%|█████████████████████████████████████████| 350/350 [00:05<00:00, 63.89it/s]\n",
      "\u001b[32m2025-05-15 10:37:16.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 351 tokens in 7.30 seconds, 48.11 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:16.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 30.69 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:16.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:16.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:37:16.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 350])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 102.32 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 2203])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you're doing great. In this video, I'm going to walk you through a basic but important concept—requests and responses, and how they help clients and servers communicate.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's get clear on what a client and server actually are. A client is what the user interacts with—like a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The server is the backend system that provides services, like APIs and databases. To make it simple, think of a restaurant. You're the client, and the ordering desk or waiter is the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order, you're making a request. When the food comes back, that's a response. That same idea applies to how the web works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you click on something or enter data on a website, the client sends a request to the server. The server processes it and sends back a response.\n",
      "\n",
      "There are a few common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A GET request is when you want to receive or fetch some information. A POST request is used when you want to send or create something new.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is for updating existing data, and DELETE is when you want to remove something. Going back to the restaurant example—GET is like reading the menu. POST is placing a new order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is changing an existing order, and DELETE is canceling the order. Each of these requests usually includes a few key parts. There are headers, which carry instructions or important details.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: There's also a body, which contains the actual data being sent. And finally, there are status codes, which tell you if the request was successful or if something went wrong.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: This entire cycle of making requests and getting responses is what keeps the internet running.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Whether you're using a mobile app, browsing a site, or using a service online, this is how the client and server talk to each other. It's a simple pattern but very powerful.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Once you understand it, you'll have a much better idea of how applications work behind the scenes. Hope that gives you a clear picture.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:11.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      "  9%|███▍                                    | 512/5918 [00:07<01:23, 64.41it/s]\n",
      "\u001b[32m2025-05-15 10:38:20.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 8.35 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:20.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 514 tokens in 8.35 seconds, 61.57 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:20.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 39.28 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:20.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:20.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:20.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 513])\u001b[0m\n",
      "  5%|██▏                                     | 289/5363 [00:04<01:19, 64.14it/s]\n",
      "\u001b[32m2025-05-15 10:38:25.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 291 tokens in 5.43 seconds, 53.54 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:25.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.16 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:25.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:25.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:25.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 290])\u001b[0m\n",
      "  8%|███▏                                    | 394/5020 [00:06<01:12, 64.01it/s]\n",
      "\u001b[32m2025-05-15 10:38:32.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 396 tokens in 6.98 seconds, 56.74 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:32.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.20 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:32.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:32.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:32.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 395])\u001b[0m\n",
      "  6%|██▎                                     | 279/4914 [00:04<01:12, 63.94it/s]\n",
      "\u001b[32m2025-05-15 10:38:38.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 281 tokens in 5.30 seconds, 53.07 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:38.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.85 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:38.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:38.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:38.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 280])\u001b[0m\n",
      "  9%|███▌                                    | 442/5030 [00:06<01:11, 63.85it/s]\n",
      "\u001b[32m2025-05-15 10:38:45.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 444 tokens in 7.75 seconds, 57.32 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:45.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.56 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:45.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:45.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:45.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 443])\u001b[0m\n",
      "  5%|█▉                                      | 255/5364 [00:03<01:19, 63.93it/s]\n",
      "\u001b[32m2025-05-15 10:38:50.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 257 tokens in 4.87 seconds, 52.79 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:50.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.68 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:50.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:50.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:50.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 256])\u001b[0m\n",
      "  9%|███▌                                    | 453/5052 [00:07<01:11, 63.95it/s]\n",
      "\u001b[32m2025-05-15 10:38:58.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 455 tokens in 7.87 seconds, 57.79 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:58.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.87 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:58.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:58.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:38:58.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 454])\u001b[0m\n",
      "  8%|███▏                                    | 434/5353 [00:06<01:16, 63.91it/s]\n",
      "\u001b[32m2025-05-15 10:39:06.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 436 tokens in 7.68 seconds, 56.75 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:06.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.20 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:06.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:06.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:06.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 435])\u001b[0m\n",
      "  9%|███▊                                    | 504/5359 [00:07<01:16, 63.85it/s]\n",
      "\u001b[32m2025-05-15 10:39:14.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 506 tokens in 8.77 seconds, 57.71 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:14.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.81 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:14.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:14.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:14.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 505])\u001b[0m\n",
      "  3%|█                                       | 140/5379 [00:02<01:22, 63.68it/s]\n",
      "\u001b[32m2025-05-15 10:39:18.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 142 tokens in 3.13 seconds, 45.42 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:18.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 28.97 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:18.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:18.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:18.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 141])\u001b[0m\n",
      "  7%|██▊                                     | 369/5188 [00:05<01:15, 63.80it/s]\n",
      "\u001b[32m2025-05-15 10:39:24.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 371 tokens in 6.45 seconds, 57.50 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:24.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.68 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:24.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:24.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:24.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 370])\u001b[0m\n",
      "  9%|███▌                                    | 430/4780 [00:06<01:08, 63.81it/s]\n",
      "\u001b[32m2025-05-15 10:39:32.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 432 tokens in 7.68 seconds, 56.22 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:32.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.86 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:32.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:32.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 431])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you're doing great. In this video, I'm going to walk you through a basic but important concept—requests and responses, and how they help clients and servers communicate.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, let's get clear on what a client and server actually are. A client is what the user interacts with—like a website or an app.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The server is the backend system that provides services, like APIs and databases. To make it simple, think of a restaurant. You're the client, and the ordering desk or waiter is the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order, you're making a request. When the food comes back, that's a response. That same idea applies to how the web works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you click on something or enter data on a website, the client sends a request to the server. The server processes it and sends back a response.\n",
      "\n",
      "There are a few common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A GET request is when you want to receive or fetch some information. A POST request is used when you want to send or create something new.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is for updating existing data, and DELETE is when you want to remove something. Going back to the restaurant example—GET is like reading the menu. POST is placing a new order.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: PUT is changing an existing order, and DELETE is canceling the order. Each of these requests usually includes a few key parts. There are headers, which carry instructions or important details.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: There's also a body, which contains the actual data being sent. And finally, there are status codes, which tell you if the request was successful or if something went wrong.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: This entire cycle of making requests and getting responses is what keeps the internet running.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Whether you're using a mobile app, browsing a site, or using a service online, this is how the client and server talk to each other. It's a simple pattern but very powerful.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Once you understand it, you'll have a much better idea of how applications work behind the scenes. Hope that gives you a clear picture.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:47.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/12 of sample 1/1\u001b[0m\n",
      "  4%|█▋                                      | 246/5918 [00:03<01:29, 63.59it/s]\n",
      "\u001b[32m2025-05-15 10:39:51.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 4.27 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:51.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 248 tokens in 4.27 seconds, 58.11 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:51.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.07 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:51.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:51.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:51.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 247])\u001b[0m\n",
      "  4%|█▌                                      | 219/5629 [00:03<01:25, 63.64it/s]\n",
      "\u001b[32m2025-05-15 10:39:56.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 221 tokens in 4.13 seconds, 53.56 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:56.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.16 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:56.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:56.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:39:56.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 220])\u001b[0m\n",
      "  9%|███▋                                    | 495/5356 [00:07<01:16, 63.57it/s]\n",
      "\u001b[32m2025-05-15 10:40:04.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 497 tokens in 8.49 seconds, 58.56 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:04.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.36 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:04.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:04.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:04.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 496])\u001b[0m\n",
      "  4%|█▍                                      | 200/5628 [00:03<01:25, 63.39it/s]\n",
      "\u001b[32m2025-05-15 10:40:08.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 202 tokens in 4.04 seconds, 50.03 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:08.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 31.92 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:08.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:08.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:08.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 201])\u001b[0m\n",
      "  5%|██                                      | 278/5375 [00:04<01:20, 63.38it/s]\n",
      "\u001b[32m2025-05-15 10:40:13.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 280 tokens in 5.06 seconds, 55.28 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:13.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.27 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:13.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:13.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:13.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 279])\u001b[0m\n",
      "  4%|█▌                                      | 198/5055 [00:03<01:16, 63.31it/s]\n",
      "\u001b[32m2025-05-15 10:40:17.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 200 tokens in 3.94 seconds, 50.72 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:17.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 32.36 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:17.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:17.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:17.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 199])\u001b[0m\n",
      "  5%|██▏                                     | 295/5375 [00:04<01:20, 63.42it/s]\n",
      "\u001b[32m2025-05-15 10:40:22.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 297 tokens in 5.33 seconds, 55.76 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:22.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.57 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:22.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:22.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:22.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 296])\u001b[0m\n",
      "  8%|███▎                                    | 415/5027 [00:06<01:12, 63.62it/s]\n",
      "\u001b[32m2025-05-15 10:40:30.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 417 tokens in 7.36 seconds, 56.68 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:30.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.16 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:30.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:30.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:30.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 416])\u001b[0m\n",
      "  6%|██▎                                     | 318/5625 [00:04<01:23, 63.77it/s]\n",
      "\u001b[32m2025-05-15 10:40:36.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 320 tokens in 5.81 seconds, 55.11 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:36.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.16 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:36.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:36.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:36.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 319])\u001b[0m\n",
      "  3%|█▏                                      | 164/5280 [00:02<01:20, 63.63it/s]\n",
      "\u001b[32m2025-05-15 10:40:39.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 166 tokens in 3.37 seconds, 49.29 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:39.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 31.44 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:39.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:39.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:39.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 165])\u001b[0m\n",
      "  4%|█▋                                      | 233/5430 [00:03<01:21, 63.51it/s]\n",
      "\u001b[32m2025-05-15 10:40:43.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 235 tokens in 4.32 seconds, 54.44 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:43.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.73 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:43.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:43.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/12 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:43.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 234])\u001b[0m\n",
      "  4%|█▍                                      | 189/5158 [00:02<01:18, 63.35it/s]\n",
      "\u001b[32m2025-05-15 10:40:47.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 191 tokens in 3.74 seconds, 51.08 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:47.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 32.58 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:47.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:40:47.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 190])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 19.74 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 425])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you are doing great.\n",
      "In this video I will explain what requests and responses are, and how clients and servers interact.\n",
      "First let's clarify what a client is.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A client is a front end interface you access, like a website or an app.\n",
      "Next a server is a back end resource providing a service, such as an API and a database.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: To make it simple, imagine you visit a restaurant. You are the client, and the ordering desk is the server.\n",
      "Now let's see how clients and servers talk to each other.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order at a restaurant, you make a request by telling the waiter what food you want.\n",
      "The waiter then brings back a response with your food.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Similarly, when you interact with a website, the client sends requests to the server.\n",
      "The server processes those requests, and sends back responses.\n",
      "There are four common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, GET is used when you want to fetch or receive information.\n",
      "Second, POST is used when you want to send or create new information.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Third, PUT is used when you want to update existing information.\n",
      "Fourth, DELETE is used when you want to remove information.\n",
      "Back in the restaurant, these map nicely.\n",
      "GET is like looking at the menu.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: POST is like placing a new order.\n",
      "PUT is like modifying an existing order.\n",
      "DELETE is like canceling your order.\n",
      "Every request and response includes three key parts.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Headers contain details and instructions, much like order notes.\n",
      "The body holds the actual data being transferred, like the food itself.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: And status codes tell you if your request succeeded or failed, similar to a yes or no from the kitchen.\n",
      "This request and response cycle is the foundation of how the internet works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: It enables applications to communicate with servers, and it keeps data flowing smoothly.\n",
      "To recap, a client is your front end interface, and a server is the back end resource.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Requests are messages sent by the client, and responses are messages sent back by the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The main request types—GET, POST, PUT, DELETE—cover fetching, creating, updating, and removing information.\n",
      "Each message includes headers, a body, and status codes.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Understanding this cycle will help you work with web applications, APIs, and services more effectively.\n",
      "Thanks for watching, and I hope this helps you grasp how clients and servers interact.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: If you have any questions, feel free to leave a comment below.\n",
      "See you next time!\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:54.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/15 of sample 1/1\u001b[0m\n",
      "  3%|█▏                                      | 217/7697 [00:03<01:55, 64.62it/s]\n",
      "\u001b[32m2025-05-15 10:45:58.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 3.46 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:58.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 219 tokens in 3.46 seconds, 63.30 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:58.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 40.38 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:58.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:58.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:45:58.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 218])\u001b[0m\n",
      "  3%|█▎                                      | 244/7431 [00:03<01:51, 64.28it/s]\n",
      "\u001b[32m2025-05-15 10:46:02.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 246 tokens in 4.11 seconds, 59.89 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:02.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 38.21 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:02.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:02.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:02.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 245])\u001b[0m\n",
      "  3%|█▏                                      | 223/7138 [00:03<01:47, 64.10it/s]\n",
      "\u001b[32m2025-05-15 10:46:06.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 225 tokens in 3.86 seconds, 58.24 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:06.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.15 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:06.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:06.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:06.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 224])\u001b[0m\n",
      "  3%|█▏                                      | 198/6870 [00:03<01:44, 63.91it/s]\n",
      "\u001b[32m2025-05-15 10:46:09.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 200 tokens in 3.51 seconds, 56.93 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:09.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.32 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:09.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:09.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:09.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 199])\u001b[0m\n",
      "  2%|▉                                       | 163/6621 [00:02<01:40, 64.04it/s]\n",
      "\u001b[32m2025-05-15 10:46:12.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 165 tokens in 2.99 seconds, 55.16 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:12.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.19 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:12.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:12.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:12.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 164])\u001b[0m\n",
      "  4%|█▋                                      | 268/6415 [00:04<01:36, 63.69it/s]\n",
      "\u001b[32m2025-05-15 10:46:17.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 270 tokens in 4.65 seconds, 58.07 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:17.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.04 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:17.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:17.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:17.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 269])\u001b[0m\n",
      "  5%|█▊                                      | 283/6085 [00:04<01:31, 63.72it/s]\n",
      "\u001b[32m2025-05-15 10:46:22.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 285 tokens in 5.04 seconds, 56.51 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:22.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.05 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:22.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:22.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:22.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 284])\u001b[0m\n",
      "  4%|█▋                                      | 244/5749 [00:03<01:26, 63.73it/s]\n",
      "\u001b[32m2025-05-15 10:46:26.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 246 tokens in 4.52 seconds, 54.45 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:26.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.73 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:26.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:26.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:26.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 245])\u001b[0m\n",
      "  4%|█▍                                      | 194/5467 [00:03<01:22, 63.72it/s]\n",
      "\u001b[32m2025-05-15 10:46:30.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 196 tokens in 3.75 seconds, 52.29 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:30.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.36 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:30.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:30.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:30.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 195])\u001b[0m\n",
      "  5%|█▊                                      | 241/5225 [00:03<01:18, 63.67it/s]\n",
      "\u001b[32m2025-05-15 10:46:35.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 243 tokens in 4.49 seconds, 54.07 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:35.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.49 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:35.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:35.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:35.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 242])\u001b[0m\n",
      "  5%|█▉                                      | 250/5230 [00:03<01:18, 63.83it/s]\n",
      "\u001b[32m2025-05-15 10:46:39.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 252 tokens in 4.68 seconds, 53.87 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:39.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.36 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:39.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:39.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:39.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 251])\u001b[0m\n",
      "  3%|█                                       | 133/5221 [00:02<01:20, 63.46it/s]\n",
      "\u001b[32m2025-05-15 10:46:42.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 135 tokens in 2.86 seconds, 47.25 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:42.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 30.14 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:42.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:42.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 13/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:42.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 134])\u001b[0m\n",
      "  5%|██                                      | 266/5031 [00:04<01:14, 63.90it/s]\n",
      "\u001b[32m2025-05-15 10:46:47.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 268 tokens in 4.86 seconds, 55.18 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:47.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.20 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:47.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:47.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 14/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:47.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 267])\u001b[0m\n",
      "  5%|█▉                                      | 251/5173 [00:03<01:17, 63.55it/s]\n",
      "\u001b[32m2025-05-15 10:46:52.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 253 tokens in 4.73 seconds, 53.52 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:52.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.14 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:52.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:52.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 15/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:52.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 252])\u001b[0m\n",
      "  2%|▊                                       | 110/5202 [00:01<01:20, 63.36it/s]\n",
      "\u001b[32m2025-05-15 10:46:54.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 112 tokens in 2.50 seconds, 44.84 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:54.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 28.60 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:54.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:46:54.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 111])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you are doing great.\n",
      "In this video I will explain what requests and responses are, and how clients and servers interact.\n",
      "First let's clarify what a client is.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A client is a front end interface you access, like a website or an app.\n",
      "Next a server is a back end resource providing a service, such as an API and a database.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: To make it simple, imagine you visit a restaurant. You are the client, and the ordering desk is the server.\n",
      "Now let's see how clients and servers talk to each other.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order at a restaurant, you make a request by telling the waiter what food you want.\n",
      "The waiter then brings back a response with your food.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Similarly, when you interact with a website, the client sends requests to the server.\n",
      "The server processes those requests, and sends back responses.\n",
      "There are four common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, GET is used when you want to fetch or receive information.\n",
      "Second, POST is used when you want to send or create new information.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Third, PUT is used when you want to update existing information.\n",
      "Fourth, DELETE is used when you want to remove information.\n",
      "Back in the restaurant, these map nicely.\n",
      "GET is like looking at the menu.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: POST is like placing a new order.\n",
      "PUT is like modifying an existing order.\n",
      "DELETE is like canceling your order.\n",
      "Every request and response includes three key parts.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Headers contain details and instructions, much like order notes.\n",
      "The body holds the actual data being transferred, like the food itself.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: And status codes tell you if your request succeeded or failed, similar to a yes or no from the kitchen.\n",
      "This request and response cycle is the foundation of how the internet works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: It enables applications to communicate with servers, and it keeps data flowing smoothly.\n",
      "To recap, a client is your front end interface, and a server is the back end resource.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Requests are messages sent by the client, and responses are messages sent back by the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The main request types—GET, POST, PUT, DELETE—cover fetching, creating, updating, and removing information.\n",
      "Each message includes headers, a body, and status codes.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Understanding this cycle will help you work with web applications, APIs, and services more effectively.\n",
      "Thanks for watching, and I hope this helps you grasp how clients and servers interact.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: If you have any questions, feel free to leave a comment below.\n",
      "See you next time!\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:09.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/15 of sample 1/1\u001b[0m\n",
      "  4%|█▌                                      | 291/7697 [00:04<01:54, 64.48it/s]\n",
      "\u001b[32m2025-05-15 10:48:14.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 4.61 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:14.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 293 tokens in 4.61 seconds, 63.50 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:14.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 40.51 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:14.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:14.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:14.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 292])\u001b[0m\n",
      "  4%|█▋                                      | 300/7357 [00:04<01:50, 63.98it/s]\n",
      "\u001b[32m2025-05-15 10:48:19.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 302 tokens in 5.09 seconds, 59.38 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:19.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.88 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:19.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:19.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:19.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 301])\u001b[0m\n",
      "  4%|█▍                                      | 250/7008 [00:03<01:45, 64.16it/s]\n",
      "\u001b[32m2025-05-15 10:48:24.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 252 tokens in 4.36 seconds, 57.83 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:24.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.89 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:24.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:24.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:24.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 251])\u001b[0m\n",
      "  3%|█▎                                      | 230/6713 [00:03<01:41, 64.08it/s]\n",
      "\u001b[32m2025-05-15 10:48:28.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 232 tokens in 4.06 seconds, 57.15 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:28.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.45 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:28.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:28.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:28.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 231])\u001b[0m\n",
      "  4%|█▊                                      | 285/6432 [00:04<01:35, 64.08it/s]\n",
      "\u001b[32m2025-05-15 10:48:33.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 287 tokens in 4.95 seconds, 57.97 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:33.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.98 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:33.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:33.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:33.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 286])\u001b[0m\n",
      "  4%|█▌                                      | 237/6104 [00:03<01:32, 63.73it/s]\n",
      "\u001b[32m2025-05-15 10:48:37.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 239 tokens in 4.33 seconds, 55.22 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:37.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.22 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:37.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:37.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:37.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 238])\u001b[0m\n",
      "  6%|██▍                                     | 354/5805 [00:05<01:25, 63.87it/s]\n",
      "\u001b[32m2025-05-15 10:48:43.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 356 tokens in 6.18 seconds, 57.65 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:43.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.78 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:43.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:43.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:43.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 355])\u001b[0m\n",
      "  6%|██▏                                     | 299/5398 [00:04<01:19, 63.76it/s]\n",
      "\u001b[32m2025-05-15 10:48:49.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 301 tokens in 5.49 seconds, 54.81 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:49.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.96 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:49.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:49.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:49.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 300])\u001b[0m\n",
      "  5%|█▊                                      | 235/5061 [00:03<01:15, 63.73it/s]\n",
      "\u001b[32m2025-05-15 10:48:53.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 237 tokens in 4.52 seconds, 52.45 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:53.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.46 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:53.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:53.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:53.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 236])\u001b[0m\n",
      "  6%|██▍                                     | 305/5127 [00:04<01:15, 63.69it/s]\n",
      "\u001b[32m2025-05-15 10:48:59.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 307 tokens in 5.55 seconds, 55.27 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:59.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.26 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:59.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:59.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:48:59.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 306])\u001b[0m\n",
      "  6%|██▎                                     | 290/5074 [00:04<01:15, 63.45it/s]\n",
      "\u001b[32m2025-05-15 10:49:04.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 292 tokens in 5.40 seconds, 54.09 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:04.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.51 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:04.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:04.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:04.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 291])\u001b[0m\n",
      "  3%|█▏                                      | 149/5028 [00:02<01:17, 63.34it/s]\n",
      "\u001b[32m2025-05-15 10:49:07.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 151 tokens in 3.19 seconds, 47.39 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:07.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 30.23 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:07.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:07.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 13/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:07.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 150])\u001b[0m\n",
      "  6%|██▏                                     | 287/5158 [00:04<01:16, 63.37it/s]\n",
      "\u001b[32m2025-05-15 10:49:12.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 289 tokens in 5.21 seconds, 55.49 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:12.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.40 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:12.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:12.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 14/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:12.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 288])\u001b[0m\n",
      "  6%|██▍                                     | 311/5102 [00:04<01:15, 63.20it/s]\n",
      "\u001b[32m2025-05-15 10:49:18.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 313 tokens in 5.74 seconds, 54.56 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:18.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.80 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:18.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:18.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 15/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:18.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 312])\u001b[0m\n",
      "  2%|▉                                       | 126/5176 [00:01<01:20, 63.00it/s]\n",
      "\u001b[32m2025-05-15 10:49:21.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 128 tokens in 2.82 seconds, 45.37 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:21.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 28.94 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:21.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:49:21.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 127])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you are doing great.\n",
      "In this video I will explain what requests and responses are, and how clients and servers interact.\n",
      "First let's clarify what a client is.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A client is a front end interface you access, like a website or an app.\n",
      "Next a server is a back end resource providing a service, such as an API and a database.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: To make it simple, imagine you visit a restaurant. You are the client, and the ordering desk is the server.\n",
      "Now let's see how clients and servers talk to each other.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order at a restaurant, you make a request by telling the waiter what food you want.\n",
      "The waiter then brings back a response with your food.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Similarly, when you interact with a website, the client sends requests to the server.\n",
      "The server processes those requests, and sends back responses.\n",
      "There are four common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, GET is used when you want to fetch or receive information.\n",
      "Second, POST is used when you want to send or create new information.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Third, PUT is used when you want to update existing information.\n",
      "Fourth, DELETE is used when you want to remove information.\n",
      "Back in the restaurant, these map nicely.\n",
      "GET is like looking at the menu.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: POST is like placing a new order.\n",
      "PUT is like modifying an existing order.\n",
      "DELETE is like canceling your order.\n",
      "Every request and response includes three key parts.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Headers contain details and instructions, much like order notes.\n",
      "The body holds the actual data being transferred, like the food itself.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: And status codes tell you if your request succeeded or failed, similar to a yes or no from the kitchen.\n",
      "This request and response cycle is the foundation of how the internet works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: It enables applications to communicate with servers, and it keeps data flowing smoothly.\n",
      "To recap, a client is your front end interface, and a server is the back end resource.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Requests are messages sent by the client, and responses are messages sent back by the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The main request types—GET, POST, PUT, DELETE—cover fetching, creating, updating, and removing information.\n",
      "Each message includes headers, a body, and status codes.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Understanding this cycle will help you work with web applications, APIs, and services more effectively.\n",
      "Thanks for watching, and I hope this helps you grasp how clients and servers interact.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: If you have any questions, feel free to leave a comment below.\n",
      "See you next time!\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:38.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/15 of sample 1/1\u001b[0m\n",
      "  3%|█▏                                      | 240/8132 [00:03<02:01, 64.82it/s]\n",
      "\u001b[32m2025-05-15 10:54:42.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 3.76 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:42.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 242 tokens in 3.76 seconds, 64.28 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:42.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 41.01 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:42.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:42.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:42.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 241])\u001b[0m\n",
      "  3%|█▏                                      | 236/7843 [00:03<01:58, 64.13it/s]\n",
      "\u001b[32m2025-05-15 10:54:45.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 238 tokens in 3.96 seconds, 60.17 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:45.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 38.38 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:45.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:45.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:45.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 237])\u001b[0m\n",
      "  3%|█                                       | 199/7558 [00:03<01:55, 63.92it/s]\n",
      "\u001b[32m2025-05-15 10:54:49.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 201 tokens in 3.42 seconds, 58.69 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:49.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.44 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:49.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:49.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:49.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 200])\u001b[0m\n",
      "  2%|▉                                       | 174/7314 [00:02<01:52, 63.74it/s]\n",
      "\u001b[32m2025-05-15 10:54:52.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 176 tokens in 3.05 seconds, 57.67 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:52.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.79 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:52.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:52.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:52.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 175])\u001b[0m\n",
      "  3%|█▍                                      | 246/7089 [00:03<01:47, 63.81it/s]\n",
      "\u001b[32m2025-05-15 10:54:56.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 248 tokens in 4.19 seconds, 59.26 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:56.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.80 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:56.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:56.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:54:56.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 247])\u001b[0m\n",
      "  3%|█                                       | 187/6800 [00:02<01:44, 63.56it/s]\n",
      "\u001b[32m2025-05-15 10:55:00.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 189 tokens in 3.38 seconds, 55.86 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:00.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.64 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:00.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:00.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:00.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 188])\u001b[0m\n",
      "  4%|█▋                                      | 267/6551 [00:04<01:38, 63.66it/s]\n",
      "\u001b[32m2025-05-15 10:55:04.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 269 tokens in 4.64 seconds, 58.03 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:04.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 37.02 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:04.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:04.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:04.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 268])\u001b[0m\n",
      "  4%|█▌                                      | 245/6231 [00:03<01:34, 63.59it/s]\n",
      "\u001b[32m2025-05-15 10:55:09.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 247 tokens in 4.45 seconds, 55.53 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:09.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.42 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:09.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:09.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:09.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 246])\u001b[0m\n",
      "  3%|█▏                                      | 180/5948 [00:02<01:30, 63.40it/s]\n",
      "\u001b[32m2025-05-15 10:55:12.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 182 tokens in 3.45 seconds, 52.72 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:12.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.63 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:12.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:12.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:12.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 181])\u001b[0m\n",
      "  4%|█▌                                      | 232/5720 [00:03<01:26, 63.49it/s]\n",
      "\u001b[32m2025-05-15 10:55:16.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 234 tokens in 4.25 seconds, 55.05 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:16.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.11 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:16.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:16.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:16.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 233])\u001b[0m\n",
      "  4%|█▋                                      | 225/5441 [00:03<01:22, 63.40it/s]\n",
      "\u001b[32m2025-05-15 10:55:21.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 227 tokens in 4.25 seconds, 53.41 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:21.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.07 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:21.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:21.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:21.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 226])\u001b[0m\n",
      "  2%|▉                                       | 123/5185 [00:01<01:20, 63.22it/s]\n",
      "\u001b[32m2025-05-15 10:55:23.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 125 tokens in 2.69 seconds, 46.48 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:23.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 29.65 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:23.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:23.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 13/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:23.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 124])\u001b[0m\n",
      "  5%|█▉                                      | 253/5290 [00:03<01:19, 63.37it/s]\n",
      "\u001b[32m2025-05-15 10:55:28.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 255 tokens in 4.64 seconds, 54.99 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:28.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.08 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:28.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:28.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 14/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:28.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 254])\u001b[0m\n",
      "  5%|█▉                                      | 239/4988 [00:03<01:14, 63.51it/s]\n",
      "\u001b[32m2025-05-15 10:55:32.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 241 tokens in 4.57 seconds, 52.72 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:32.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.63 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:32.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:32.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 15/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:32.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 240])\u001b[0m\n",
      "  2%|▊                                        | 98/4966 [00:01<01:17, 63.11it/s]\n",
      "\u001b[32m2025-05-15 10:55:35.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 100 tokens in 2.35 seconds, 42.52 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:35.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 27.13 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:35.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:55:35.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 99])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 59.83 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 1289])\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hey everyone, hope you are doing great.\n",
      "In this video I will explain what requests and responses are, and how clients and servers interact.\n",
      "First let's clarify what a client is.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: A client is a front end interface you access, like a website or an app.\n",
      "Next a server is a back end resource providing a service, such as an API and a database.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: To make it simple, imagine you visit a restaurant. You are the client, and the ordering desk is the server.\n",
      "Now let's see how clients and servers talk to each other.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: When you place an order at a restaurant, you make a request by telling the waiter what food you want.\n",
      "The waiter then brings back a response with your food.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Similarly, when you interact with a website, the client sends requests to the server.\n",
      "The server processes those requests, and sends back responses.\n",
      "There are four common types of requests.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: First, GET is used when you want to fetch or receive information.\n",
      "Second, POST is used when you want to send or create new information.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Third, PUT is used when you want to update existing information.\n",
      "Fourth, DELETE is used when you want to remove information.\n",
      "Back in the restaurant, these map nicely.\n",
      "GET is like looking at the menu.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: POST is like placing a new order.\n",
      "PUT is like modifying an existing order.\n",
      "DELETE is like canceling your order.\n",
      "Every request and response includes three key parts.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Headers contain details and instructions, much like order notes.\n",
      "The body holds the actual data being transferred, like the food itself.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: And status codes tell you if your request succeeded or failed, similar to a yes or no from the kitchen.\n",
      "This request and response cycle is the foundation of how the internet works.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: It enables applications to communicate with servers, and it keeps data flowing smoothly.\n",
      "To recap, a client is your front end interface, and a server is the back end resource.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Requests are messages sent by the client, and responses are messages sent back by the server.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: The main request types—GET, POST, PUT, DELETE—cover fetching, creating, updating, and removing information.\n",
      "Each message includes headers, a body, and status codes.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Understanding this cycle will help you work with web applications, APIs, and services more effectively.\n",
      "Thanks for watching, and I hope this helps you grasp how clients and servers interact.\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: If you have any questions, feel free to leave a comment below.\n",
      "See you next time!\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:42.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/15 of sample 1/1\u001b[0m\n",
      "  3%|█▏                                      | 213/6833 [00:03<01:43, 64.26it/s]\n",
      "\u001b[32m2025-05-15 10:57:45.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 3.55 seconds\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:45.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 215 tokens in 3.55 seconds, 60.61 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:45.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 38.66 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:45.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:45.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:45.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 214])\u001b[0m\n",
      "  3%|█▎                                      | 214/6571 [00:03<01:39, 63.87it/s]\n",
      "\u001b[32m2025-05-15 10:57:49.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 216 tokens in 3.81 seconds, 56.76 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:49.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 36.21 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:49.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:49.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:49.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 215])\u001b[0m\n",
      "  3%|█▏                                      | 182/6308 [00:02<01:36, 63.78it/s]\n",
      "\u001b[32m2025-05-15 10:57:53.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 184 tokens in 3.36 seconds, 54.73 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:53.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.91 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:53.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:53.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:53.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 183])\u001b[0m\n",
      "  3%|█                                       | 163/6081 [00:02<01:32, 63.69it/s]\n",
      "\u001b[32m2025-05-15 10:57:56.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 165 tokens in 3.09 seconds, 53.48 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:56.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.11 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:56.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:56.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:57:56.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 164])\u001b[0m\n",
      "  4%|█▌                                      | 223/5867 [00:03<01:28, 63.63it/s]\n",
      "\u001b[32m2025-05-15 10:58:00.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 225 tokens in 4.06 seconds, 55.48 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:00.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.39 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:00.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:00.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:00.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 224])\u001b[0m\n",
      "  3%|█▏                                      | 166/5601 [00:02<01:25, 63.54it/s]\n",
      "\u001b[32m2025-05-15 10:58:03.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 168 tokens in 3.27 seconds, 51.38 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:03.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 32.77 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:03.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:03.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 7/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:03.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 167])\u001b[0m\n",
      "  5%|█▉                                      | 254/5373 [00:04<01:20, 63.37it/s]\n",
      "\u001b[32m2025-05-15 10:58:08.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 256 tokens in 4.66 seconds, 54.98 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:08.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 35.07 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:08.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:08.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 8/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:08.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 255])\u001b[0m\n",
      "  4%|█▋                                      | 210/5066 [00:03<01:16, 63.32it/s]\n",
      "\u001b[32m2025-05-15 10:58:12.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 212 tokens in 4.11 seconds, 51.61 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:12.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 32.92 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:12.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:12.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 9/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:12.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 211])\u001b[0m\n",
      "  3%|█▎                                      | 165/5081 [00:02<01:17, 63.29it/s]\n",
      "\u001b[32m2025-05-15 10:58:15.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 167 tokens in 3.36 seconds, 49.71 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:15.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 31.71 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:15.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:15.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 10/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:15.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 166])\u001b[0m\n",
      "  4%|█▋                                      | 212/5099 [00:03<01:17, 63.32it/s]\n",
      "\u001b[32m2025-05-15 10:58:19.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 214 tokens in 4.05 seconds, 52.79 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:19.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.68 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:19.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:19.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 11/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:19.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 213])\u001b[0m\n",
      "  4%|█▋                                      | 218/5048 [00:03<01:16, 63.40it/s]\n",
      "\u001b[32m2025-05-15 10:58:23.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 220 tokens in 4.20 seconds, 52.36 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:23.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.40 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:23.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:23.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 12/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:23.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 219])\u001b[0m\n",
      "  2%|▊                                       | 109/5073 [00:01<01:18, 63.21it/s]\n",
      "\u001b[32m2025-05-15 10:58:26.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 111 tokens in 2.48 seconds, 44.73 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:26.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 28.54 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:26.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:26.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 13/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:26.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 110])\u001b[0m\n",
      "  5%|█▊                                      | 235/5116 [00:03<01:16, 63.42it/s]\n",
      "\u001b[32m2025-05-15 10:58:30.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 237 tokens in 4.38 seconds, 54.12 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:30.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 34.53 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:30.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:30.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 14/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:30.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 236])\u001b[0m\n",
      "  4%|█▊                                      | 228/5148 [00:03<01:17, 63.41it/s]\n",
      "\u001b[32m2025-05-15 10:58:35.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 230 tokens in 4.35 seconds, 52.92 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:35.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 33.76 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:35.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:35.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 15/15 of sample 1/1\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:35.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 229])\u001b[0m\n",
      "  2%|▋                                        | 90/5152 [00:01<01:20, 62.96it/s]\n",
      "\u001b[32m2025-05-15 10:58:37.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 92 tokens in 2.18 seconds, 42.20 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:37.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 26.92 GB/s\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:37.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 5.43 GB\u001b[0m\n",
      "\u001b[32m2025-05-15 10:58:37.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 91])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python tools/run_webui.py \\\n",
    "    --llama-checkpoint-path checkpoints/fish-speech-1.5 \\\n",
    "    --decoder-checkpoint-path checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth \\\n",
    "    --compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break-down CLI Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encode reference audio: / 从语音生成 prompt: \n",
    "\n",
    "You should get a `fake.npy` file.\n",
    "\n",
    "你应该能得到一个 `fake.npy` 文件."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "## Enter the path to the audio file here\n",
    "src_audio = r\"D:\\PythonProject\\vo_hutao_draw_appear.wav\"\n",
    "\n",
    "!python fish_speech/models/vqgan/inference.py \\\n",
    "    -i {src_audio} \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "audio = Audio(filename=\"fake.wav\")\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate semantic tokens from text: / 从文本生成语义 token:\n",
    "\n",
    "> This command will create a codes_N file in the working directory, where N is an integer starting from 0.\n",
    "\n",
    "> You may want to use `--compile` to fuse CUDA kernels for faster inference (~30 tokens/second -> ~300 tokens/second).\n",
    "\n",
    "> 该命令会在工作目录下创建 codes_N 文件, 其中 N 是从 0 开始的整数.\n",
    "\n",
    "> 您可以使用 `--compile` 来融合 cuda 内核以实现更快的推理 (~30 tokens/秒 -> ~300 tokens/秒)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python fish_speech/models/text2semantic/inference.py \\\n",
    "    --text \"hello world\" \\\n",
    "    --prompt-text \"The text corresponding to reference audio\" \\\n",
    "    --prompt-tokens \"fake.npy\" \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5\" \\\n",
    "    --num-samples 2\n",
    "    # --compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate speech from semantic tokens: / 从语义 token 生成人声:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python fish_speech/models/vqgan/inference.py \\\n",
    "    -i \"codes_0.npy\" \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "audio = Audio(filename=\"fake.wav\")\n",
    "display(audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
